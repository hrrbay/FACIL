# What is this repository?

This is a fork of [FACIL](https://github.com/mmasana/FACIL) implementing some QoL changes while keeping the same funcitonality.

# Changes

## Implement FACIL as a self-contained package

For easier access of variables previously defined in `main_incremental.py` and easier relative imports, code is changed such that everything is contained in a package called `facil`.
Therefore, the folder structure is changed to

```
src/
    facil/
        approach/
        __init__.py
        ...
    main_incremental.py
```
where the previous code of `main_incremental.py` is moved into `facil.py`.
Many variables (e.g. loaders, args) are then easily accessible within the whole package, i.e., from any newly added file via `facil.x`. While this may not always be desirable, it can be particularly useful when implementing new approaches and the need to access some variables (otherwise not needed) arises to perform debugging, sanity checking or similar.

## approach constructors and arguments

Up until now, adding new arguments to an approach was somewhat tedious due to the redundancy of defining default values in `extra_parser` and `__init__`. This is also suspect to confusion: which default value is actually used? 

All default arguments are therefore set in the super-constructor (i.e. `Inc_Learning_Appr:__init__`) in this version. Additionally, all arguments to a sub-class are passed as double asterisk keyword-arguments. This fully removes the requirement of manually adding approach-arguments to its corresponding constructor as defaults (including member-names) are derived from its `extra_parser`-function. Therefore, we now only need to define new parameters in `extra_parser`, they will be set automatically in the super-constructor!

> NOTE: This change led to some renaming of used members as they are now exactly the same as generated by the parser.

This is implemented as follows:
``` python 

# Inc_Learning_appr (super-class)
def __init__(self, model, device, nepochs, lr, lr_min, lr_factor, lr_patience, clipgrad,
                 momentum, wd, multi_softmax, wu_nepochs, wu_lr_factor, fix_bn,
                 eval_on_train, exemplars_dataset: ExemplarsDataset, **appr_kwargs):
        ... # base-args are set here

        # set approach-specific default arguments
        default_args = self.extra_parser(None)[0].__dict__
        for k, v in default_args.items():
            setattr(self, k, v)

        # overwrite approach-specific defaults with passed kwargs
        for k, v in appr_kwargs.items():
            assert k in default_args.keys(), f'Argument "{k}" is not defined in {self.__class__.__name__}.extra_parser!'
            setattr(self, k, v)

# sub-class (e.g. dmc)
def __init__(self, model, device, **kwargs):
        super(Appr, self).__init__(model, device, **kwargs)
        self.model_old = None
        self.model_new = None
        
        # get dataloader for auxiliar dataset
        aux_trn_ldr, _, aux_val_ldr, _ = get_loaders([self.aux_dataset], num_tasks=1, nc_first_task=None, validation=0,
                                                     batch_size=self.aux_batch_size, num_workers=4, pin_memory=False)
        self.aux_trn_loader = aux_trn_ldr[0]
        self.aux_val_loader = aux_val_ldr[0]
        # Since an auxiliary dataset is available, using exemplars could be redundant
        have_exemplars = self.exemplars_dataset.max_num_exemplars + self.exemplars_dataset.max_num_exemplars_per_class
        assert (have_exemplars == 0), 'Warning: DMC does not use exemplars. Comment this line to force it.'

# sub-class `extra_parser` (e.g. dmc) -- new arguments only need to be defined here
@staticmethod
def extra_parser(args):
    """Returns a parser containing the approach specific parameters"""
    parser = ArgumentParser()
    # Sec. 4.2.1 "We use ImageNet32x32 dataset as the source for auxiliary data in the model consolidation stage."
    parser.add_argument('--aux-dataset', default='imagenet_32_reduced', type=str, required=False,
                        help='Auxiliary dataset (default=%(default)s)')
    parser.add_argument('--aux-batch-size', default=128, type=int, required=False,
                        help='Batch size for auxiliary dataset (default=%(default)s)')
    return parser.parse_known_args(args)

```




### changes
- log gs-args
- change np.float to float
- package!!
- logging
- lr-list
- approach args